{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of using BeautifulSoup4 to scrape abstracts from *all* the pages directly linked to from http://www.branchcollective.org/?page_id=13 .  The BS4 documentation is at https://www.crummy.com/software/BeautifulSoup/bs4/doc/.  You'll need to make sure that you install beautifulsoup, likely with:\n",
    "\n",
    "    pip install beautifulsoup\n",
    "\n",
    "If it turns out that you don't have `pip` installed then look at https://pip.pypa.io/en/stable/installing/.  If it looks like you installed beautiful soup but you still get an error message when you run this code then there are likely multiple python environments on the system and we may need to sort that out tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see about just grabbing web content. Sample code for testing from https://docs.python.org/3/howto/urllib2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen('http://python.org/') as response:\n",
    "    html = response.read()\n",
    "    print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see about just using BS4 to grab abstract content from a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with urllib.request.urlopen('http://www.branchcollective.org/?ps_articles=erika-rappaport-object-lessons-and-colonial-histories-inventing-the-jubilee-of-indian-tea') as response:\n",
    "    soup = BeautifulSoup(response.read(), 'html.parser')\n",
    "    #print(soup.prettify())\n",
    "    print(soup.find(\"div\", class_=\"article_extract\").get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can extract an article text let's look to how we can crawl the entire site.  We could use a prebuilt webscraper for this but let's build one from scratch instead.  And use BS4 to do it. Our goal here is to collect ALL the pages that we might find abstracts on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with urllib.request.urlopen('http://www.branchcollective.org/?page_id=13') as response:\n",
    "    soup = BeautifulSoup(response.read(), 'html.parser')\n",
    "    \n",
    "    #we first find all the list items with the class \"cat-item\"\n",
    "    listItems = soup.find_all(\"li\", class_=\"cat-item\")\n",
    "    \n",
    "    #we then iterate over the responses to get the hrefs.\n",
    "    topicURLs = []\n",
    "    for item in listItems:\n",
    "        #print(item.find('a').get('href')))\n",
    "        topicURLs.append(item.find('a').get('href'))\n",
    "    \n",
    "    #print(topicURLs)\n",
    "    \n",
    "    #we take the topic URLs and for each we load the page and extract the main article pages it links to\n",
    "    #by recognizing that each of these links is inside a div tag with the ps_articles class.\n",
    "    categoryURLs = []\n",
    "    for url in topicURLs:\n",
    "         with urllib.request.urlopen(url) as response2:\n",
    "            soup = BeautifulSoup(response2.read(), 'html.parser')\n",
    "            divItems = soup.find_all(\"div\", class_=\"ps_articles\")\n",
    "            for div in divItems:\n",
    "                categoryURLs.append(item.find('a').get('href'))\n",
    "    print(categoryURLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can get all the URLs we can wrap everything up by combining our ability to get the url from one file with a loop through all the urls.  I'll get you started but you'll need to finish it. ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with urllib.request.urlopen('http://www.branchcollective.org/?page_id=13') as response:\n",
    "    soup = BeautifulSoup(response.read(), 'html.parser')\n",
    "    \n",
    "    #we first find all the list items with the class \"cat-item\"\n",
    "    listItems = soup.find_all(\"li\", class_=\"cat-item\")\n",
    "    \n",
    "    #we then iterate over the responses to get the hrefs.\n",
    "    topicURLs = []\n",
    "    for item in listItems:\n",
    "        #print(item.find('a').get('href')))\n",
    "        topicURLs.append(item.find('a').get('href'))\n",
    "    \n",
    "    #print(topicURLs)\n",
    "    \n",
    "    #we take the topic URLs and for each we load the page and extract the main article pages it links to\n",
    "    #by recognizing that each of these links is inside a div tag with the ps_articles class.\n",
    "    categoryURLs = []\n",
    "    for url in topicURLs:\n",
    "         with urllib.request.urlopen(url) as response2:\n",
    "            soup = BeautifulSoup(response2.read(), 'html.parser')\n",
    "            divItems = soup.find_all(\"div\", class_=\"ps_articles\")\n",
    "            for div in divItems:\n",
    "                categoryURLs.append(item.find('a').get('href'))\n",
    "    #print(categoryURLs)\n",
    "    \n",
    "    Here is where your loop will go.  Each time through will need to substitute a url from categoryURLs for the\n",
    "    url used in the original test that was run.  Oh, and you'll need to figure out how to output this.\n",
    "    And you'll need to delete this text because this cell well crash otherwise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
