{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of using BeautifulSoup4 to scrape abstracts from *all* the pages directly linked to from http://www.branchcollective.org/?page_id=13 .  The BS4 documentation is at https://www.crummy.com/software/BeautifulSoup/bs4/doc/.  You'll need to make sure that you install beautifulsoup, likely with:\n",
    "\n",
    "    pip install beautifulsoup\n",
    "\n",
    "If it turns out that you don't have `pip` installed then look at https://pip.pypa.io/en/stable/installing/.  If it looks like you installed beautiful soup but you still get an error message when you run this code then there are likely multiple python environments on the system and we may need to sort that out tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see about just grabbing web content. Sample code for testing from https://docs.python.org/3/howto/urllib2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen('http://python.org/') as response:\n",
    "    html = response.read()\n",
    "    print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see about just using BS4 to grab abstract content from a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"poem\" data-view=\"ContentView\"><div style=\"text-indent: -1em; padding-left: 1em;\">Now I know that I’ve never described<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " anything, not one single thing, not<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " the flesh of the avocado which darkens<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " so quickly, though if you scrape<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " what’s been exposed to the air it’s new-green<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " beneath like nothing ever happened.<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " I want to describe this evening, though<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " it’s not spectacular. The baby babbling<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " in the other room over the din<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " and whistle of a football game, and now<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " the dog just outside the door, scratching,<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " rattling the tags on her collar, the car<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " going by, far away but loud, a car without<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " a muffler, and the sound of the baby<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " returning again, pleasure and weight.<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " I want to describe the baby. I want to describe<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " the baby for many hours to anyone<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " who wishes to hear me. My feelings for her<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " take me so far inside myself I can see the pure<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " holiness in motherhood, and it makes me<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " burn with success and fear, the hole her<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " coming has left open, widening. Last night<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " we fed her some of the avocado I’ve just<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " finished eating while writing this poem.<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " Her first food. I thought my heart might burst,<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " knowing she would no longer be made<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " entirely of me, flesh of my flesh. Startled<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " in her amusing way by the idea of eating,<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " she tried to take it in, but her mouth<br/></div><div style=\"text-indent: -1em; padding-left: 1em;\">\r",
      " pushed it out. And my heart did burst. <br/></div><div style=\"text-indent: -1em; padding-left: 1em;\"><br/></div></div>]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with urllib.request.urlopen('http://www.poetryfoundation.org/poems-and-poets/poems/detail/89655') as response:\n",
    "    soup = BeautifulSoup(response.read(), 'html.parser')\n",
    "    #print(soup.prettify())\n",
    "    print(soup.find_all(class_=\"poem\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can extract an article text let's look to how we can crawl the entire site.  We could use a prebuilt webscraper for this but let's build one from scratch instead.  And use BS4 to do it. Our goal here is to collect ALL the pages that we might find abstracts on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with urllib.request.urlopen('http://www.branchcollective.org/?page_id=13') as response:\n",
    "    soup = BeautifulSoup(response.read(), 'html.parser')\n",
    "    \n",
    "    #we first find all the list items with the class \"cat-item\"\n",
    "    listItems = soup.find_all(\"li\", class_=\"cat-item\")\n",
    "    \n",
    "    #we then iterate over the responses to get the hrefs.\n",
    "    topicURLs = []\n",
    "    for item in listItems:\n",
    "        #print(item.find('a').get('href')))\n",
    "        topicURLs.append(item.find('a').get('href'))\n",
    "    \n",
    "    #print(topicURLs)\n",
    "    \n",
    "    #we take the topic URLs and for each we load the page and extract the main article pages it links to\n",
    "    #by recognizing that each of these links is inside a div tag with the ps_articles class.\n",
    "    categoryURLs = []\n",
    "    for url in topicURLs:\n",
    "         with urllib.request.urlopen(url) as response2:\n",
    "            soup = BeautifulSoup(response2.read(), 'html.parser')\n",
    "            divItems = soup.find_all(\"div\", class_=\"ps_articles\")\n",
    "            for div in divItems:\n",
    "                categoryURLs.append(item.find('a').get('href'))\n",
    "    print(categoryURLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can get all the URLs we can wrap everything up by combining our ability to get the url from one file with a loop through all the urls.  I'll get you started but you'll need to finish it. ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with urllib.request.urlopen('http://www.branchcollective.org/?page_id=13') as response:\n",
    "    soup = BeautifulSoup(response.read(), 'html.parser')\n",
    "    \n",
    "    #we first find all the list items with the class \"cat-item\"\n",
    "    listItems = soup.find_all(\"li\", class_=\"cat-item\")\n",
    "    \n",
    "    #we then iterate over the responses to get the hrefs.\n",
    "    topicURLs = []\n",
    "    for item in listItems:\n",
    "        #print(item.find('a').get('href')))\n",
    "        topicURLs.append(item.find('a').get('href'))\n",
    "    \n",
    "    #print(topicURLs)\n",
    "    \n",
    "    #we take the topic URLs and for each we load the page and extract the main article pages it links to\n",
    "    #by recognizing that each of these links is inside a div tag with the ps_articles class.\n",
    "    categoryURLs = []\n",
    "    for url in topicURLs:\n",
    "         with urllib.request.urlopen(url) as response2:\n",
    "            soup = BeautifulSoup(response2.read(), 'html.parser')\n",
    "            divItems = soup.find_all(\"div\", class_=\"ps_articles\")\n",
    "            for div in divItems:\n",
    "                categoryURLs.append(item.find('a').get('href'))\n",
    "    #print(categoryURLs)\n",
    "    \n",
    "    Here is where your loop will go.  Each time through will need to substitute a url from categoryURLs for the\n",
    "    url used in the original test that was run.  Oh, and you'll need to figure out how to output this.\n",
    "    And you'll need to delete this text because this cell well crash otherwise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
